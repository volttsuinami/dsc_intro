{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WNK7vbHo-KYU"
   },
   "source": [
    "## Bayesian methods of hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlFdvPwF-KYW"
   },
   "source": [
    "In addition to the random search and the grid search methods for selecting optimal hyperparameters, we can use Bayesian methods of probabilities to select the optimal hyperparameters for an algorithm.\n",
    "\n",
    "In this case study, we will be using the BayesianOptimization library to perform hyperparmater tuning. This library has very good documentation which you can find here: https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "You will need to install the Bayesian optimization module. Running a cell with an exclamation point in the beginning of the command will run it as a shell command — please do this to install this module from our notebook in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pssx080d-Ulf"
   },
   "outputs": [],
   "source": [
    "\n",
    "#! pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T16:39:09.312682Z",
     "start_time": "2019-04-22T16:39:09.309208Z"
    },
    "_kg_hide-input": true,
    "colab": {},
    "colab_type": "code",
    "id": "l9nfFTyj-KYY"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from catboost import CatBoostClassifier, cv, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "D16Dquw1AAK0",
    "outputId": "44167587-f22e-4bf5-a816-e2bcfdc6c4ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " '.ipynb_checkpoints',\n",
       " 'Bayesian_optimization_case_study.ipynb',\n",
       " 'flight_delays_test.csv',\n",
       " 'flight_delays_train.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:48:15.929012Z",
     "start_time": "2019-04-22T14:48:15.926574Z"
    },
    "colab_type": "text",
    "id": "AkBt3yds-KYu"
   },
   "source": [
    "## How does Bayesian optimization work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E1kyBCUs-KYv"
   },
   "source": [
    "Bayesian optimization works by constructing a posterior distribution of functions (Gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAdHF72R-KYw"
   },
   "source": [
    "<img src=\"https://github.com/fmfn/BayesianOptimization/blob/master/examples/bo_example.png?raw=true\" />\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation while taking into account what it knows about the target function. At each step, a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with an exploration strategy (such as UCB — aka Upper Confidence Bound), or EI (Expected Improvement). This process is used to determine the next point that should be explored (see the gif below).\n",
    "<img src=\"https://github.com/fmfn/BayesianOptimization/raw/master/examples/bayesian_optimization.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTP8KUlLoYzu"
   },
   "source": [
    "## Let's look at a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crpPqKdC-KYx"
   },
   "source": [
    "The first step is to create an optimizer. It uses two items:\n",
    "* function to optimize\n",
    "* bounds of parameters\n",
    "\n",
    "The function is the procedure that counts metrics of our model quality. The important thing is that our optimization will maximize the value on function. Smaller metrics are best. Hint: don't forget to use negative metric values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e09ciF8gpTfr"
   },
   "source": [
    "Here we define our simple function we want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofwvnfEwo5mG"
   },
   "outputs": [],
   "source": [
    "def simple_func(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCGsdciCpeI3"
   },
   "source": [
    "Now, we define our bounds of the parameters to optimize, within the Bayesian optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4jLYW2qnpOFr"
   },
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    simple_func,\n",
    "    {'a': (1, 3),\n",
    "    'b': (4, 7)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dg6LdYx8pq2T"
   },
   "source": [
    "These are the main parameters of this function:\n",
    "\n",
    "* **n_iter:** This is how many steps of Bayesian optimization you want to perform. The more steps, the more likely you are to find a good maximum.\n",
    "\n",
    "* **init_points:** This is how many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-GKMJ1uqMYv"
   },
   "source": [
    "Let's run an example where we use the optimizer to find the best values to maximize the target value for a and b given the inputs of 3 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Oy44Ro7wqNat",
    "outputId": "9cc64d54-b1e6-46d1-dc29-4c0039a1c72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     a     |     b     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m8.885    \u001b[0m | \u001b[0m2.683    \u001b[0m | \u001b[0m6.202    \u001b[0m |\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m6.638    \u001b[0m | \u001b[0m2.289    \u001b[0m | \u001b[0m4.349    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m6.849    \u001b[0m | \u001b[0m1.141    \u001b[0m | \u001b[0m5.708    \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m6.401    \u001b[0m | \u001b[0m1.228    \u001b[0m | \u001b[0m5.173    \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m10.0     \u001b[0m | \u001b[95m3.0      \u001b[0m | \u001b[95m7.0      \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer.maximize(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyKFMF2Hq2Sx"
   },
   "source": [
    "Great, now let's print the best parameters and the associated maximized target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_H6DixyfscV_",
    "outputId": "fd0c35d7-e30d-4d30-9ab2-12c0fa837971"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3.0, 'b': 7.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(optimizer.max['params']);optimizer.max['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQ1T1V6Mspi4"
   },
   "source": [
    "## Test it on real data using the Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_oGwREZkm4h"
   },
   "source": [
    "The dataset we will be working with is the famous flight departures dataset. Our modeling goal will be to predict if a flight departure is going to be delayed by 15 minutes based on the other attributes in our dataset. As part of this modeling exercise, we will use Bayesian hyperparameter optimization to identify the best parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYSagjQANDZ"
   },
   "source": [
    "**<font color='teal'> You can load the zipped csv files just as you would regular csv files using Pandas read_csv. In the next cell load the train and test data into two seperate dataframes. </font>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWKBApVuAeJe"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('flight_delays_train.csv')\n",
    "test_df = pd.read_csv('flight_delays_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OapNcT9Eikis"
   },
   "source": [
    "**<font color='teal'> Print the top five rows of the train dataframe and review the columns in the data. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "__4cXZ8iiYaC",
    "outputId": "8718ad4b-8955-486c-9ae8-1dee6aa6c2fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>dep_delayed_15min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c-8</td>\n",
       "      <td>c-21</td>\n",
       "      <td>c-7</td>\n",
       "      <td>1934</td>\n",
       "      <td>AA</td>\n",
       "      <td>ATL</td>\n",
       "      <td>DFW</td>\n",
       "      <td>732</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c-4</td>\n",
       "      <td>c-20</td>\n",
       "      <td>c-3</td>\n",
       "      <td>1548</td>\n",
       "      <td>US</td>\n",
       "      <td>PIT</td>\n",
       "      <td>MCO</td>\n",
       "      <td>834</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c-9</td>\n",
       "      <td>c-2</td>\n",
       "      <td>c-5</td>\n",
       "      <td>1422</td>\n",
       "      <td>XE</td>\n",
       "      <td>RDU</td>\n",
       "      <td>CLE</td>\n",
       "      <td>416</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c-11</td>\n",
       "      <td>c-25</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1015</td>\n",
       "      <td>OO</td>\n",
       "      <td>DEN</td>\n",
       "      <td>MEM</td>\n",
       "      <td>872</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c-10</td>\n",
       "      <td>c-7</td>\n",
       "      <td>c-6</td>\n",
       "      <td>1828</td>\n",
       "      <td>WN</td>\n",
       "      <td>MDW</td>\n",
       "      <td>OMA</td>\n",
       "      <td>423</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Month DayofMonth DayOfWeek  DepTime UniqueCarrier Origin Dest  Distance  \\\n",
       "0   c-8       c-21       c-7     1934            AA    ATL  DFW       732   \n",
       "1   c-4       c-20       c-3     1548            US    PIT  MCO       834   \n",
       "2   c-9        c-2       c-5     1422            XE    RDU  CLE       416   \n",
       "3  c-11       c-25       c-6     1015            OO    DEN  MEM       872   \n",
       "4  c-10        c-7       c-6     1828            WN    MDW  OMA       423   \n",
       "\n",
       "  dep_delayed_15min  \n",
       "0                 N  \n",
       "1                 N  \n",
       "2                 N  \n",
       "3                 N  \n",
       "4                 Y  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxGBsPQhffgd"
   },
   "source": [
    "**<font color='teal'> Use the describe function to review the numeric columns in the train dataframe. </font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "_bRRKG3DAtae",
    "outputId": "7cfb9975-ec97-422c-abbd-98923a0b7aec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DepTime</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1341.523880</td>\n",
       "      <td>729.39716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>476.378445</td>\n",
       "      <td>574.61686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>30.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>931.000000</td>\n",
       "      <td>317.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1330.000000</td>\n",
       "      <td>575.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1733.000000</td>\n",
       "      <td>957.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2534.000000</td>\n",
       "      <td>4962.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             DepTime      Distance\n",
       "count  100000.000000  100000.00000\n",
       "mean     1341.523880     729.39716\n",
       "std       476.378445     574.61686\n",
       "min         1.000000      30.00000\n",
       "25%       931.000000     317.00000\n",
       "50%      1330.000000     575.00000\n",
       "75%      1733.000000     957.00000\n",
       "max      2534.000000    4962.00000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i6k-_fI5Aiyh"
   },
   "source": [
    "Notice, `DepTime` is the departure time in a numeric representation in 2400 hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gtZS4-hrlQah"
   },
   "source": [
    " **<font color='teal'>The response variable is 'dep_delayed_15min' which is a categorical column, so we need to map the Y for yes and N for no values to 1 and 0. Run the code in the next cell to do this.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:38:42.677690Z",
     "start_time": "2019-04-22T15:38:42.481963Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "yRlOTbnW-KYc"
   },
   "outputs": [],
   "source": [
    "#train_df = train_df[train_df.DepTime <= 2400].copy()\n",
    "y_train = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z3WPkFQO9uo9"
   },
   "source": [
    "## Feature Engineering\n",
    "Use these defined functions to create additional features for the model. Run the cell to add the functions to your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXqsqz5W9t3r"
   },
   "outputs": [],
   "source": [
    "def label_enc(df_column):\n",
    "    df_column = LabelEncoder().fit_transform(df_column)\n",
    "    return df_column\n",
    "\n",
    "def make_harmonic_features_sin(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.sin(value)\n",
    "\n",
    "def make_harmonic_features_cos(value, period=2400):\n",
    "    value *= 2 * np.pi / period \n",
    "    return np.cos(value)\n",
    "\n",
    "def feature_eng(df):\n",
    "    df['flight'] = df['Origin']+df['Dest']\n",
    "    df['Month'] = df.Month.map(lambda x: x.split('-')[-1]).astype('int32')\n",
    "    df['DayofMonth'] = df.DayofMonth.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['begin_of_month'] = (df['DayofMonth'] < 10).astype('uint8')\n",
    "    df['midddle_of_month'] = ((df['DayofMonth'] >= 10)&(df['DayofMonth'] < 20)).astype('uint8')\n",
    "    df['end_of_month'] = (df['DayofMonth'] >= 20).astype('uint8')\n",
    "    df['DayOfWeek'] = df.DayOfWeek.map(lambda x: x.split('-')[-1]).astype('uint8')\n",
    "    df['hour'] = df.DepTime.map(lambda x: x/100).astype('int32')\n",
    "    df['morning'] = df['hour'].map(lambda x: 1 if (x <= 11)& (x >= 7) else 0).astype('uint8')\n",
    "    df['day'] = df['hour'].map(lambda x: 1 if (x >= 12) & (x <= 18) else 0).astype('uint8')\n",
    "    df['evening'] = df['hour'].map(lambda x: 1 if (x >= 19) & (x <= 23) else 0).astype('uint8')\n",
    "    df['night'] = df['hour'].map(lambda x: 1 if (x >= 0) & (x <= 6) else 0).astype('int32')\n",
    "    df['winter'] = df['Month'].map(lambda x: x in [12, 1, 2]).astype('int32')\n",
    "    df['spring'] = df['Month'].map(lambda x: x in [3, 4, 5]).astype('int32')\n",
    "    df['summer'] = df['Month'].map(lambda x: x in [6, 7, 8]).astype('int32')\n",
    "    df['autumn'] = df['Month'].map(lambda x: x in [9, 10, 11]).astype('int32')\n",
    "    df['holiday'] = (df['DayOfWeek'] >= 5).astype(int) \n",
    "    df['weekday'] = (df['DayOfWeek'] < 5).astype(int)\n",
    "    df['airport_dest_per_month'] = df.groupby(['Dest', 'Month'])['Dest'].transform('count')\n",
    "    df['airport_origin_per_month'] = df.groupby(['Origin', 'Month'])['Origin'].transform('count')\n",
    "    df['airport_dest_count'] = df.groupby(['Dest'])['Dest'].transform('count')\n",
    "    df['airport_origin_count'] = df.groupby(['Origin'])['Origin'].transform('count')\n",
    "    df['carrier_count'] = df.groupby(['UniqueCarrier'])['Dest'].transform('count')\n",
    "    df['carrier_count_per month'] = df.groupby(['UniqueCarrier', 'Month'])['Dest'].transform('count')\n",
    "    df['deptime_cos'] = df['DepTime'].map(make_harmonic_features_cos)\n",
    "    df['deptime_sin'] = df['DepTime'].map(make_harmonic_features_sin)\n",
    "    df['flightUC'] = df['flight']+df['UniqueCarrier']\n",
    "    df['DestUC'] = df['Dest']+df['UniqueCarrier']\n",
    "    df['OriginUC'] = df['Origin']+df['UniqueCarrier']\n",
    "    return df.drop('DepTime', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-BYbxXpU-FGE"
   },
   "source": [
    "Concatenate the training and testing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cj6bfSNw_RAf"
   },
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df.drop('dep_delayed_15min', axis=1), test_df])\n",
    "full_df = feature_eng(full_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSO8JbfM_W-F"
   },
   "source": [
    "Apply the earlier defined feature engineering functions to the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x6RfAINftjwi"
   },
   "outputs": [],
   "source": [
    "for column in ['UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']:\n",
    "    full_df[column] = label_enc(full_df[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IJAw1RGB_ZuM"
   },
   "source": [
    "\n",
    "Split the new full dataframe into X_train and X_test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "15cPtQU5tjfz"
   },
   "outputs": [],
   "source": [
    "X_train = full_df[:train_df.shape[0]]\n",
    "X_test = full_df[train_df.shape[0]:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "umfAw-9JErLV"
   },
   "source": [
    "Create a list of the categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T14:31:58.412296Z",
     "start_time": "2019-04-22T14:31:58.409088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "5ibeVyNb-KZI"
   },
   "outputs": [],
   "source": [
    "categorical_features = ['Month',  'DayOfWeek', 'UniqueCarrier', 'Origin', 'Dest','flight',  'flightUC', 'DestUC', 'OriginUC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzMIsMPIETVk"
   },
   "source": [
    "Let's build a light GBM model to test the bayesian optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:18:04.466965Z",
     "start_time": "2019-04-22T15:18:04.457992Z"
    },
    "colab_type": "text",
    "id": "2hfm1i5G-KZH"
   },
   "source": [
    "### [LightGBM](https://lightgbm.readthedocs.io/en/latest/) is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "* Faster training speed and higher efficiency.\n",
    "* Lower memory usage.\n",
    "* Better accuracy.\n",
    "* Support of parallel and GPU learning.\n",
    "* Capable of handling large-scale data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jf-3F2Wg-KZL"
   },
   "source": [
    "First, we define the function we want to maximize and that will count cross-validation metrics of lightGBM for our parameters.\n",
    "\n",
    "Some params such as num_leaves, max_depth, min_child_samples, min_data_in_leaf should be integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:40:14.034265Z",
     "start_time": "2019-04-22T15:40:14.027868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "LyUJBhGX-KZM"
   },
   "outputs": [],
   "source": [
    "def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples, min_data_in_leaf):\n",
    "    params = {\n",
    "        \"objective\" : \"binary\",\n",
    "        \"metric\" : \"auc\", \n",
    "        'is_unbalance': True,\n",
    "        \"num_leaves\" : int(num_leaves),\n",
    "        \"max_depth\" : int(max_depth),\n",
    "        \"lambda_l2\" : lambda_l2,\n",
    "        \"lambda_l1\" : lambda_l1,\n",
    "        \"num_threads\" : 20,\n",
    "        \"min_child_samples\" : int(min_child_samples),\n",
    "        'min_data_in_leaf': int(min_data_in_leaf),\n",
    "        \"learning_rate\" : 0.03,\n",
    "        \"subsample_freq\" : 5,\n",
    "        \"bagging_seed\" : 42,\n",
    "        \"verbosity\" : -1\n",
    "    }\n",
    "    lgtrain = lightgbm.Dataset(X_train, y_train,categorical_feature=categorical_features)\n",
    "    cv_result = lightgbm.cv(params,\n",
    "                       lgtrain,\n",
    "                       1000,\n",
    "                       stratified=True,\n",
    "                       nfold=3)\n",
    "    print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJwqBhdeF11Q"
   },
   "source": [
    "Apply the Bayesian optimizer to the function we created in the previous step to identify the best hyperparameters. We will run 10 iterations and set init_points = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:48:04.682447Z",
     "start_time": "2019-04-22T15:40:14.641634Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JheCOkUE-KZP",
    "outputId": "8f37ee51-885d-44e4-cdcd-ceb7abd58b61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | lambda_l1 | lambda_l2 | max_depth | min_ch... | min_da... | num_le... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "{'valid auc-mean': [0.6974101388319478, 0.7041766073395846, 0.7071705360752044, 0.7082007226561026, 0.7096460705180947, 0.7111772828054393, 0.712001628654645, 0.7129085352204007, 0.7134868703298046, 0.7143639666482238, 0.7146974338619908, 0.7149430177715314, 0.7152810935408033, 0.7156191645337415, 0.7158166890222253, 0.7162536680495464, 0.7165849243891526, 0.7167369856049373, 0.7170852936115097, 0.7174838725986105, 0.717797222720344, 0.7180934099312225, 0.7184040609836542, 0.7186838851385368, 0.7189019896204081, 0.7191182594608908, 0.7192922540495981, 0.7194275115916889, 0.7196854277664033, 0.7197779396638562, 0.719925671346812, 0.7202065061797774, 0.7204088861484402, 0.7205620054133428, 0.7207192458499998, 0.7208559533297972, 0.7209941869932742, 0.7210745523992542, 0.721228444331432, 0.7213647587495227, 0.7214697845191503, 0.7215517335930764, 0.7216922272196764, 0.721791309991552, 0.7219693840092991, 0.7221115318969084, 0.7222455154847319, 0.7223475059193508, 0.7225263577076421, 0.7226696862024776, 0.7227903074008571, 0.7229148489945519, 0.7230337851653146, 0.7232249199182181, 0.7233393725232317, 0.723517244226557, 0.7236145659230308, 0.723752589472498, 0.7239421124025119, 0.7241035936437711, 0.7241954593755482, 0.7243408936923168, 0.7244255570605546, 0.7245616057769063, 0.7246352992885274, 0.7247495169220315, 0.7248939872634953, 0.7250401842552678, 0.7251472160514595, 0.7253119963390091, 0.7253866564784479, 0.7255270268904725, 0.7257168174841643, 0.7258267116055337, 0.7259403775375297, 0.7261758584946132, 0.7262340760688913, 0.7264471251208576, 0.7266658249109322, 0.7268385748627978, 0.7269531898624343, 0.7270720763915303, 0.727245488642024, 0.7273402135849768, 0.7275108459879823, 0.727622016008216, 0.7278436497926649, 0.72801616318121, 0.7281320501513262, 0.7282301668149191, 0.7284002475427359, 0.728524444798497, 0.7286374446033856, 0.7288077028666131, 0.7289425461462163, 0.7291026501063707, 0.7292258296504238, 0.7293590798649134, 0.729463539614596, 0.7295873494047029, 0.7297918368085616, 0.7298833548903959, 0.7300329081316143, 0.7301575631152994, 0.7302901368359199, 0.7303938847661602, 0.7305223822874393, 0.7306238015444384, 0.7307446695075216, 0.7308664329371517, 0.7309084500935638, 0.7309779487601483, 0.7311145337747121, 0.7311782358918295, 0.7312451211422388, 0.7313749224618125, 0.731474379949317, 0.731563290910184, 0.7316334316419386, 0.7317414689180142, 0.7318793254374593, 0.7319658797275884, 0.7320598319670686, 0.7321180085904094, 0.7321868321370623, 0.7322701580727878, 0.7323386881163018, 0.7324089147721636, 0.7325059778735684, 0.7325878522859778, 0.7326409062264093, 0.7327153078601568, 0.7327589693283555, 0.7328273721842026, 0.7329019666121169, 0.732977198394702, 0.7330452316223619, 0.733102749494407, 0.733167478964484, 0.7332646114134018, 0.7333384235555037, 0.7334014816417261, 0.7334596604221649, 0.7335443811307428, 0.7335920823167896, 0.7336489296431915, 0.7337548295262941, 0.7338555067929602, 0.7338869696992202, 0.7339167670776261, 0.7339654798505997, 0.7340277772114989, 0.7341125587986275, 0.7341517683146602, 0.7342105938149578, 0.7343018080459999, 0.7343889647013927, 0.7344812913971376, 0.7345296345368763, 0.7345907853980814, 0.734636098669034, 0.7346723852544823, 0.734709331701683, 0.7347811972771269, 0.7348429044737971, 0.7349183652444413, 0.7349466955432593, 0.734982053892657, 0.7350041950336413, 0.735065566043238, 0.735110175320223, 0.7351710281095531, 0.7352583428670769, 0.7353272631347204, 0.7353787824543455, 0.7354388480348266, 0.7355001195217649, 0.735563949677851, 0.7355964674435865, 0.735626702740019, 0.7356551532831211, 0.7357180089819849, 0.7357628844856162, 0.7358366823573044, 0.7358896846763558, 0.7359386252969351, 0.7359807007066053, 0.7360439053250101, 0.736092303376109, 0.7361634016967447, 0.736196909944313, 0.7362380244733192, 0.7362747099634954, 0.736356474216009, 0.7364016902356201, 0.7364186000667345, 0.7364944172345472, 0.7365521532390723, 0.7366349634278935, 0.7366771051135824, 0.7367363900471481, 0.7367860860381258, 0.7368188214981437, 0.7368493443096856, 0.7368888359269193, 0.7369190687276364, 0.7369500447161051, 0.7369690383802722, 0.7369909023181792, 0.7370326964589142, 0.7370864005793583, 0.7371481572721822, 0.7371865684701125, 0.7372166671239165, 0.7372444931012749, 0.7372941635311269, 0.7373361808666544, 0.7373474863496723, 0.7373905390872393, 0.7374070711901064, 0.7374259055017971, 0.7374858949137492, 0.7375295705084707, 0.737565721009836, 0.7375688850720444, 0.7376057458953217, 0.7376396215554277, 0.7376672240392522, 0.7376994945008836, 0.7377108293510091, 0.7377385737281191, 0.7377664833327423, 0.7377796802562853, 0.7378198276241301, 0.7378571758998045, 0.7378598068685496, 0.737884128269162, 0.7379161306321625, 0.7379340005836745, 0.737966764953422, 0.7379807735653262, 0.7380356386758423, 0.7380696275178003, 0.738085416460461, 0.7381020871449288, 0.7381420435612438, 0.7381580541820583, 0.7381754428538341, 0.738214905159555, 0.7382376394219001, 0.7382526480531618, 0.7382757844603166, 0.7382825562806676, 0.7383220709039673, 0.738333648939701, 0.738360789939085, 0.7383553044769823, 0.7383707997451042, 0.7384268936381336, 0.7384429566549077, 0.738485379153884, 0.7385476107093386, 0.7385714185884305, 0.7385962556758098, 0.738617641353954, 0.7386636553148334, 0.7386931004245901, 0.7387225356861107, 0.7387271864345338, 0.7387502371775291, 0.7387669560169497, 0.7387696412378478, 0.738778493042005, 0.7387952060503024, 0.7388343609279021, 0.7388445861695061, 0.7388670844311864, 0.7388975358050329, 0.7389157178356408, 0.7389295454635172, 0.7389374227125302, 0.738982046474557, 0.7390038753196543, 0.7390349370829674, 0.7390584297324034, 0.7390997894836241, 0.7391258214941789, 0.7391397033752178, 0.7391589503044061, 0.7391546774727199, 0.7392074471604048, 0.739231435692767, 0.7392472285058737, 0.7392756345739612, 0.7392723482074993, 0.7393082084073019, 0.7393285758499125, 0.739348748692278, 0.739360873461632, 0.7393727219528327, 0.7393825679890655, 0.7393872321658659, 0.7394018163286059, 0.7394235013591114, 0.7394501229754221, 0.739463890004093, 0.7395004097087154, 0.7395179538085128, 0.7395376188661641, 0.7395552755280489, 0.7395766666594725, 0.739595175644356, 0.7396101683800795, 0.7396348129364849, 0.7396805899138729, 0.7396796870413733, 0.7396913329629274, 0.7396992680691835, 0.7397245624726084, 0.7397204002226472, 0.7397283723747466, 0.7397424491128058, 0.7397516220012493, 0.7397735866737988, 0.7397872390047722, 0.7398099358303787, 0.7398357772437404, 0.7398492738209822, 0.7398811529403838, 0.7398962627374912, 0.7398980004458929, 0.7398972899301556, 0.7399015979023869, 0.7399149680304156, 0.7399304592288581, 0.739941706331925, 0.7399463823684744, 0.739943134589588, 0.7399725325280118, 0.7399887435574808, 0.7400101521237948, 0.7400275832685185, 0.7400441814634581, 0.7400574562317429, 0.7400751482087452, 0.7400872397528757, 0.7401123202689378, 0.7401293525606348, 0.7401344548460417, 0.7401530864640611, 0.7401605743309446, 0.7401808951368997, 0.7401895757289756, 0.7402252611755661, 0.7402547433295078, 0.7402646747433446, 0.7402925005883857, 0.7402962618688909, 0.7403173765853731, 0.7403488960889405, 0.7403619390667053, 0.7403749182326554, 0.7403781425178124, 0.7403827327130341, 0.7403969744366855, 0.7404195158622483, 0.7404403700217586, 0.7404605586068661, 0.7404562076707785, 0.7404735959001744, 0.7404773066445185, 0.7404856758504885, 0.7404883336345986, 0.7405174305342812, 0.7405161464218954, 0.7405471872980957, 0.7405754746456762, 0.7405754331001759, 0.7405887041724428, 0.7406297836189507, 0.7406326714303343, 0.7406345044294729, 0.740650888514958, 0.7406597073733807, 0.7406790142600174, 0.7406875511776786, 0.7406815541867481, 0.7407021357843849, 0.7407111198333296, 0.7407258459287981, 0.7407401755009636, 0.7407396870324986, 0.7407637436374399, 0.740767639179293, 0.7407761525647194, 0.7408012252181714, 0.740812318342002, 0.7408172124033029, 0.7408115091751708, 0.7408263563321906, 0.740837300013866, 0.7408506351297013, 0.740860313673522, 0.7408785872709432, 0.7408865145771067, 0.7408908481367829, 0.7409186508351483, 0.740932628079176, 0.7409586717659663, 0.740979074421734, 0.7409864296553147, 0.7409956530038458, 0.7410022844981695, 0.741013354564161, 0.7410263391779107, 0.7410419973910525, 0.7410483388573895, 0.7410566319940507, 0.7410693305343372, 0.7410854560095389, 0.7410924940147137, 0.7410830022106912, 0.741086284773839, 0.7411013789262707, 0.7411069733334007, 0.7411096898623981, 0.7411327736856809, 0.7411417851260578, 0.741159517804541, 0.7411567993991602, 0.7411867037252136, 0.741191587429415, 0.7412009956439573, 0.7412021865265128, 0.7411979214945585, 0.7411955264197593, 0.7411930317623082, 0.7412157890059864, 0.7412305094875196, 0.7412370474028681, 0.7412478432331531, 0.7412580065191356, 0.7412584967581299, 0.7412830204701718, 0.7413014012754764, 0.7413204533510335, 0.7413158297785133, 0.741341054208171, 0.7413335665028083, 0.7413481567024668, 0.741355986962867, 0.7413570239816306, 0.7413723984860775, 0.7413878547016023, 0.7414074516306469, 0.7413960644637357, 0.7414169301851908, 0.7414110925856029, 0.7414318216950754, 0.7414387800158279, 0.7414392255200037, 0.7414437105554853, 0.7414627858515562, 0.7414668663708985, 0.7414729548246632, 0.7414886170544855, 0.7414985078495485, 0.7414974766369588, 0.7415044003328797, 0.7415284121466147, 0.7415503460062993, 0.7415744727113406, 0.7415823610119209, 0.7415758714883984, 0.7415742214649709, 0.7415789752923977, 0.7416006308045858, 0.7416023334963954, 0.7416030395284837, 0.7416111011622618, 0.7416188184486961, 0.7416326070997606, 0.7416408188851605, 0.7416455392362852, 0.7416480726706663, 0.741662320185998, 0.7416787314160912, 0.7416838004638001, 0.7417051466062984, 0.7417103827612926, 0.7417109626638206, 0.741725833044383, 0.7417340697257163, 0.7417311644960268, 0.7417186855634693, 0.7417137234089667, 0.7417356454890859, 0.741743251870537, 0.7417536852431564, 0.7417587809951124, 0.7417571135923171, 0.7417552902302837, 0.741757833644209, 0.7417653134113208, 0.7417678060942503, 0.7417632160870852, 0.7417676489117793, 0.7417840758679577, 0.7417991681316035, 0.7418062917079188, 0.7418308970859712, 0.7418422160692577, 0.7418447458120193, 0.7418687518042213, 0.7418593763780749, 0.7418690281709951, 0.7418619568780795, 0.7418691079564402, 0.7418695435424355, 0.7418746512407418, 0.7418759358064678, 0.7419029052705847, 0.7419068944751297, 0.7419116305241454, 0.7419260219333875, 0.7419338283691567, 0.7419354746763762, 0.7419361441579281, 0.741956015628848, 0.7419667469506152, 0.7419604168919126, 0.7419894179528407, 0.7420017626139556, 0.7420073143401794, 0.7420120526003741, 0.742007294796503, 0.7420168179040099, 0.742035550738855, 0.7420335367054299, 0.7420286389705365, 0.7420292344377009, 0.7420308611594214, 0.7420380391008822, 0.7420396656718258, 0.7420464896136231, 0.7420643875854536, 0.7420625875856977, 0.7420552419029817, 0.742055726280928, 0.7420647668799495, 0.7420740387989172, 0.7420741031280316, 0.7420756986965399, 0.7420727430128919, 0.7420747236349118, 0.7420773172967842, 0.7420706721624475, 0.7420828883568514, 0.7420864377217032, 0.7420781074469011, 0.7420773698425834, 0.7420769903924792, 0.7420839953197462, 0.7420843533849791, 0.7420841762941836, 0.7420899344703787, 0.7420866577494589, 0.7421023843962224, 0.7421093115021825, 0.7421110280961164, 0.7421185646636627, 0.7421213494304375, 0.7421221434455928, 0.7421281037391756, 0.7421186426955956, 0.7421292612648392, 0.742127389168469, 0.7421236629416912, 0.7421295374587712, 0.7421314815918345, 0.742126529442979, 0.7421291972203335, 0.7421268970395353, 0.7421271226508689, 0.7421426835003974, 0.7421415235962655, 0.7421415331119414, 0.742165216258269, 0.7421713185646198, 0.7421693084835551, 0.7421721476004771, 0.7421878116332771, 0.7421891251735574, 0.7421956690413141, 0.7422009540233035, 0.7422165133668654, 0.7422186265873604, 0.7422234912431921, 0.7422304107009903, 0.7422452146736488, 0.742247368786782, 0.7422545527932111, 0.7422698958436126, 0.7422601489163569, 0.7422838943137307, 0.7422831588106277, 0.7422762526782373, 0.7422505225850541, 0.7422456231355147, 0.7422545176656454, 0.7422586837910283, 0.742259328154916, 0.7422547941446186, 0.7422513675107528, 0.7422552298798045, 0.7422526594401044, 0.7422433406997415, 0.7422448701134438, 0.742233362307171, 0.7422407723400567, 0.7422444210098019, 0.7422532961120608, 0.7422575326812083, 0.7422641371565352, 0.7422504539038149, 0.7422511310207658, 0.7422562448141089, 0.7422582434206954, 0.7422534292515589, 0.7422661456655106, 0.7422762951790186, 0.7422836482861469, 0.7422897856840094, 0.7423108731855064, 0.7423126790303023, 0.7423122449603688, 0.7423044070787026, 0.7423101589091429, 0.7423099214857953, 0.7423026962712576, 0.7423053386048054, 0.7423156555605384, 0.7423215459941397, 0.7423367218356565, 0.7423361476893319, 0.7423461729138975, 0.742349124804759, 0.742349969267455, 0.7423610061822054, 0.7423710120366575, 0.7423826268063691, 0.7423770519861081, 0.742380663522832, 0.7423765498540512, 0.7423912705695734, 0.742393798192733, 0.7424093631181422, 0.7424100635375931, 0.7424102641063266, 0.742414618946246, 0.7424373411356201, 0.7424514351745217, 0.742438966132888, 0.7424605010543869, 0.742456319319012, 0.7424644413936475, 0.7424763462994352, 0.7424792941630886, 0.7424884590615801, 0.7424915297464262, 0.7425047403521652, 0.7425147126600101, 0.7425219611053732, 0.7425201593560713, 0.7425158823782715, 0.7425129034328642, 0.7425154389151726, 0.7425156822272573, 0.7425244268585031, 0.7425328563841901, 0.7425400367963447, 0.7425469892746529, 0.7425488321252005, 0.7425561799011994, 0.7425646968736169, 0.7425711065113205, 0.7425812757287281, 0.7425871524055022, 0.7425915948345647, 0.7425965335318243, 0.7426043422763339, 0.7426096779761234, 0.7426118730130452, 0.7426252626786934, 0.7426349550928389, 0.7426470895064371, 0.7426534273713613, 0.7426739756874587, 0.7426711288799753, 0.7426835280862164, 0.7426866668696722, 0.7426735265156031, 0.7426758534736214, 0.7426833159424867, 0.7426707437056116, 0.7426722537166794, 0.7426818741077604, 0.742680936424536, 0.7426897415706217, 0.742693226578021, 0.7427016188610107, 0.7427046307301598, 0.7426957381258781, 0.7426914065924969, 0.7426950122370944, 0.7427036830628485, 0.7426921244107124, 0.7426903789917758, 0.7426922021833468, 0.7426911514225378, 0.7427110536868535, 0.7427149883876623, 0.7427121181708874, 0.7427189657072927, 0.7427194794441264, 0.7427199561343237, 0.7427184343077085, 0.7427246008565723, 0.7427160665518011, 0.7427334196981219, 0.742740911441076, 0.742742380788231, 0.7427412480917291, 0.7427484614898545, 0.7427463560639721, 0.7427530946756123, 0.7427438791306349, 0.7427566264729677, 0.7427573328717231, 0.7427541727595676, 0.742761304541797, 0.7427731313623639, 0.7427835553192481, 0.7427751142959362, 0.7427862895630577, 0.7428043724647867, 0.7428093538530475, 0.7428131929309919, 0.742808336210815, 0.7428205796830097, 0.7428239694917534, 0.7428184336710905, 0.7428286378431497, 0.7428385481723709, 0.7428419066715009, 0.7428356507931054, 0.7428350554306412, 0.7428303425270752, 0.7428289396063553, 0.7428409185620795, 0.7428515956161185, 0.7428569369976242, 0.7428662091851206, 0.7428653199016542, 0.7428698595720356, 0.7428728015224353, 0.7428740546499077, 0.7428791293002309, 0.7428801159546659, 0.7428784542565893, 0.7428803981587642, 0.7428720988399601, 0.7428785903432682, 0.7428855762082488, 0.7428907676009517, 0.742885903011531, 0.7428864421752263, 0.7428888102055028, 0.7428845388611794, 0.7428788100892995, 0.7428769111474233, 0.742886416485749, 0.7428887866780269, 0.7428848017180223, 0.7428929841490776, 0.742895996569633, 0.7429032236894693, 0.7429026926892167, 0.7429009239948202, 0.7428956331066434, 0.7429002292973217, 0.7428980287208796, 0.7429089800949188, 0.7429008095048646, 0.7429137029199824, 0.7429278881995547, 0.7429347025023718, 0.7429506490320991, 0.7429417526211864, 0.7429537468819595, 0.7429608513682271, 0.742961980093909, 0.7429656774090105, 0.7429661367574801, 0.7429692132882316, 0.7429742705079185, 0.7429870857709718, 0.742985221657487, 0.7429820676520498, 0.7429781894835075, 0.7429735740896448, 0.7429799020719376, 0.7429906336644686, 0.742997504698497, 0.7430021204524649, 0.743015439797292, 0.7430070472816109, 0.7430166522587603, 0.7430136402544818, 0.7430156560438705, 0.7430114315271436, 0.7430244648815757, 0.743019037752482, 0.7430299073248762, 0.7430354335604233, 0.7430342856816717, 0.7430291875567275, 0.7430376949788574, 0.743032517030756, 0.7430290068568759, 0.743029736580052, 0.7430379012682454, 0.743038183384805, 0.7430345057235604, 0.743039195284318, 0.7430315890126867, 0.743048031560686, 0.7430458075360143, 0.7430500826887713, 0.7430516742302878, 0.743055542511728, 0.7430652543588648, 0.7430700879723137, 0.743063828096851, 0.7430740577600915, 0.743069662201313, 0.7430684634920571, 0.7430638010817482, 0.7430724972243276, 0.7430839489577311, 0.7430781619816984, 0.7430866675370353, 0.7430903044796372, 0.7431041766984053, 0.7431067159939063, 0.7431086676817524, 0.7431220107594175, 0.7431199170296723, 0.7431101856579674, 0.7431103782226157, 0.7431124251796385, 0.7431242034957474, 0.7431162741935741, 0.7431199246006792, 0.7431239893976859, 0.743125534532498, 0.743133029948107, 0.7431291382309121, 0.7431262799040995, 0.7431292103900436, 0.7431374746699744, 0.7431312030044185, 0.7431311117533642, 0.743131827785331, 0.7431292612306429, 0.7431290819867874, 0.7431364954754286, 0.7431424030703443, 0.7431330416505175, 0.7431357619871554, 0.7431312865791887, 0.7431323276403751, 0.7431272469532778, 0.7431228201368031, 0.7431200551731476, 0.7431223278141162, 0.743124298959975, 0.7431234290350127, 0.7431302569130183, 0.7431199068480107, 0.7431209653691809, 0.7431148687937958, 0.743107157314545, 0.7431117866894175, 0.743110743715866, 0.7431195622562945, 0.7431297178610176, 0.743131611175114, 0.7431297919762835, 0.7431272486266783, 0.7431268515410804, 0.7431283985866709, 0.7431268340788612, 0.7431304847294014, 0.74312805634184, 0.7431386960890783, 0.7431433292621451, 0.7431350609946655, 0.7431463121501061, 0.7431461525500124, 0.7431445455464686, 0.7431588066463627, 0.7431458955689995, 0.74315490700916, 0.7431610848689623, 0.7431653502000909, 0.7431721692936993, 0.7431733465332132, 0.7431751133562018, 0.743187329613123, 0.7431823792976747, 0.7431770028110286, 0.743173202425504, 0.7431793747495358, 0.743174944256512, 0.7431691786213651, 0.7431676977422619, 0.7431563338228329, 0.7431351918179603, 0.743132870479069, 0.7431317942726506, 0.7431265871192508, 0.7431404436774075, 0.7431631559368471, 0.7431607041258612, 0.7431544619368724, 0.7431541060066728, 0.743149961506712, 0.7431414074063926, 0.7431550732519118, 0.7431466961834038, 0.7431698463693787, 0.7431601791902164, 0.7431681921169861, 0.7431753470989624, 0.7431712160181493, 0.7431744366067834, 0.7431759193707063, 0.7431605917283953, 0.7431673555426425, 0.7431675539899709, 0.7431632886187507, 0.7431657269284585, 0.743179667212595, 0.743183749485882, 0.7431802041101655, 0.7431843663114542, 0.743191246925767, 0.7431977634935961, 0.743200915745449, 0.7431960395098617, 0.7431912021626106, 0.7431947651853581, 0.7432032122719074, 0.7432025080573627, 0.7432064464328456, 0.7432058684884709, 0.7432006204346452, 0.7431927007416715, 0.7431972929843879, 0.7431940744090694, 0.7431957324817934, 0.7431965827576579, 0.7431943254895915, 0.7431970381314139, 0.7431881787341127, 0.7431809926089468, 0.7431810799953956, 0.743188589032414, 0.7431831641158692, 0.743189400809756, 0.7431821778663905, 0.7431935320383176, 0.74317733647888, 0.7431738552000192, 0.7431816016625484, 0.7431816834029018, 0.7431821073721506, 0.7431903326271735, 0.7431816831500928, 0.7431899627862689, 0.7431920643352153, 0.7431914222422115, 0.7431914417084445, 0.7431840727837266, 0.7431798969504287, 0.7431710586514434, 0.7431613177135871, 0.7431587629845412, 0.743160829434765], 'valid auc-stdv': [0.004515548649985032, 0.004697596358156726, 0.006221002987212036, 0.0054866946974367016, 0.005550950129669883, 0.005772443810438429, 0.005394832279787401, 0.005638727876405301, 0.005591833386278517, 0.005706619007327485, 0.005552485300150674, 0.0055405373668752774, 0.005764890099521799, 0.005815801318343324, 0.005838708206713017, 0.005953956247274635, 0.0058339448340982885, 0.005862153271415145, 0.005855145965182686, 0.005792402915685923, 0.005655078610179738, 0.005565666462539855, 0.005451106922091813, 0.00544210713679534, 0.005385390604279061, 0.005448987327360564, 0.005437310567258705, 0.00547882428419688, 0.005410001235922307, 0.005390279281074335, 0.005397552129604943, 0.005492497005865571, 0.00549400136361238, 0.005515547817590568, 0.005577941666842632, 0.005549552723045951, 0.005475855830236588, 0.005420191383562161, 0.0054353529373099025, 0.00536767631179306, 0.005332351466724609, 0.0053871007618195455, 0.005396402838920186, 0.005432369936723374, 0.0054658448958010445, 0.005439679903572865, 0.005402681891394472, 0.005388513538741563, 0.005444230608843987, 0.005366441229027887, 0.0053333630533075405, 0.0053803809468494215, 0.005445042146765769, 0.005423130738345717, 0.005456645810309967, 0.005446925981196668, 0.005463792616444268, 0.005395610149833483, 0.005300583325831545, 0.005259144454746302, 0.005194323529258166, 0.005194366471051794, 0.005162919034156542, 0.0051418981248443855, 0.005156104599455412, 0.005098902950751577, 0.005101604231883066, 0.005059912760723921, 0.005101775016666608, 0.005113539231353013, 0.005115090679579779, 0.005121003153874209, 0.005071848903989571, 0.005076129731991864, 0.005092732930312008, 0.005066647550144268, 0.005064710250424493, 0.005041830321573793, 0.005021497650904143, 0.0050060904524444555, 0.005048183231857507, 0.00505211515286211, 0.00503261933032599, 0.004975799700130074, 0.004963084336819218, 0.004977484800832849, 0.00508023306787294, 0.005127122256778746, 0.005145521592158301, 0.005131010145930008, 0.005247374105345291, 0.005339059022329275, 0.005386350394508966, 0.005303675046197399, 0.005295241601370056, 0.00537749657030938, 0.005364926850692674, 0.005303534043820718, 0.005335934075222725, 0.005278558436747826, 0.005291316905792159, 0.005299542178620844, 0.005271595207856419, 0.0052307944989617525, 0.005254674740811031, 0.005252194058615874, 0.005241419873939999, 0.005272881458450352, 0.005258138702856993, 0.005222388486987021, 0.005223714205674691, 0.005230537914182695, 0.00520518490863986, 0.005196681006971227, 0.005208550944663447, 0.005252925459299929, 0.005241755409116197, 0.00522443399949523, 0.0052331498020804015, 0.0052563166406500664, 0.005290261775720659, 0.005287333169977468, 0.0053032794953863345, 0.005306784691557436, 0.005299155314654476, 0.005360019393096777, 0.0053377915285996915, 0.005326268029895754, 0.005374117509615161, 0.005375717744061166, 0.0053905857567842375, 0.005432018603586571, 0.005427568258134032, 0.005448367563509045, 0.005481890591403881, 0.005447852094289723, 0.0054612558309014405, 0.005469311907939769, 0.005469188618089513, 0.005499466995539381, 0.0055332002725932495, 0.005521791809214247, 0.005513013848330338, 0.005545143313099211, 0.005538726360597848, 0.005499022969490869, 0.005514885669561158, 0.005523359575327696, 0.005514047116526854, 0.005500337542240156, 0.005509691568025021, 0.005492556739720071, 0.005495988398176292, 0.005483999815203618, 0.005489882933839546, 0.00546421199871298, 0.005515035769838934, 0.005463038430383943, 0.005464805072484912, 0.005470965386233313, 0.005485702067574663, 0.005487078271589492, 0.005474282193224489, 0.005435544488893272, 0.005451481839381815, 0.005456340991555057, 0.005438970359716271, 0.0054424948094045815, 0.005439943554115294, 0.005433725947432695, 0.005419084629642215, 0.005442758243698062, 0.005407280947557699, 0.0053818462605920115, 0.005357744595675844, 0.005330644089859676, 0.005336111173780656, 0.005352746066116184, 0.005338192807957982, 0.005317036092300573, 0.005317891607217897, 0.0053061077914179165, 0.0053182285330511565, 0.005277852406619177, 0.005237777940378621, 0.005241336730678174, 0.005248100069455281, 0.0052620012273920805, 0.00524176448193327, 0.005237560432814104, 0.0052157252043417346, 0.005202600841351536, 0.005204741260411832, 0.005163048351995099, 0.00517611680609807, 0.00516431177174251, 0.005149370200089391, 0.005145324394716725, 0.0051614396747763575, 0.005164372773704507, 0.005159952834184737, 0.005138556232396391, 0.005133943382881559, 0.0051417980940595895, 0.005136613483482053, 0.00514818205427462, 0.005169329187004071, 0.005167652822888243, 0.005159090006275818, 0.005117097123869302, 0.0051146728127633495, 0.005081090977019616, 0.0050955402195170755, 0.005098549727410998, 0.0051013113475515135, 0.005098063880109978, 0.005086809669610749, 0.005089721546726349, 0.005070030137804016, 0.005071429308774148, 0.005062493169707699, 0.0050525491535890425, 0.005017894400390029, 0.00501235687664689, 0.005010256567305108, 0.005004292725487315, 0.005002655878365981, 0.004988028665025453, 0.004976020257627111, 0.00496965514130706, 0.004964811083201329, 0.004962276599423426, 0.004958119590644596, 0.004950601170752882, 0.004943109712439441, 0.004932675212887967, 0.004935631096398909, 0.004904688121703964, 0.004899763171985593, 0.0049103682690699755, 0.0048974861239817705, 0.004932644225281899, 0.004915085278759097, 0.004910802222736334, 0.004889092429841463, 0.004893132049320837, 0.004891265628863762, 0.004870040185435791, 0.0048656774883735375, 0.004827960963118746, 0.004823381715924577, 0.004825151514717028, 0.004815202693037355, 0.004826662804754859, 0.004820293651329657, 0.004822876451092966, 0.00482704134476507, 0.004811901937903224, 0.004795331509131934, 0.004803265191017742, 0.004771406381661995, 0.0047386457937379, 0.004726992674004824, 0.004729114588499186, 0.004697802039712458, 0.004700875242290994, 0.00469299670150945, 0.0046866124587279494, 0.004680012077763891, 0.004687000683697777, 0.004692799580624665, 0.0046993467553518254, 0.004693076772589136, 0.004699112861679066, 0.004697763612145815, 0.004716011486805234, 0.004710911622881622, 0.004690223326144168, 0.004703209149687903, 0.004689190509486531, 0.004667081210430772, 0.004664012529401671, 0.004659911915920923, 0.004668205970296893, 0.00465729016044028, 0.004646329124791963, 0.004638451103375652, 0.004637926722674256, 0.004615792262635185, 0.004603581636268057, 0.004630534562171849, 0.004635144167915427, 0.004625773823790899, 0.004611917785211135, 0.004605216390820119, 0.004618681279854225, 0.004619622571536276, 0.004614090268935876, 0.004611331143400673, 0.004604810257881782, 0.004610442864375345, 0.004613361920950632, 0.0046254713772740635, 0.0046124859507920505, 0.0046032009659575145, 0.004605106534640182, 0.004625809388045558, 0.004618534950258511, 0.0046152088517182, 0.004623851255042194, 0.004614794496757247, 0.0046205537128535664, 0.00463697688118016, 0.004631329929546105, 0.004609454124805973, 0.004615449980706579, 0.004623322671053542, 0.004630285186988259, 0.00463215958454621, 0.004634765835524282, 0.0046400282643806485, 0.004623146424372839, 0.004618922153851575, 0.004637493325766515, 0.004632975189791691, 0.00461871830239151, 0.004603151745152972, 0.004600914241870237, 0.004606509930891569, 0.00460025128086614, 0.004592364743483427, 0.004608987105469824, 0.004617121006843196, 0.004613658013434499, 0.004607805203856789, 0.004606803939590287, 0.004599437353055041, 0.004607093037059041, 0.004620988505452193, 0.004620837278420974, 0.004615147297264837, 0.00461107134908479, 0.004615427573438242, 0.004612010992977662, 0.004605900891090812, 0.004614091639025382, 0.004606054342305839, 0.004600478506851057, 0.004590186452971585, 0.004594976782348885, 0.004586400774974957, 0.004580389181817136, 0.004580629867120184, 0.0045734601442030485, 0.0045617785332365286, 0.004580695226817178, 0.004585812689116522, 0.004591340014340126, 0.004584359382469296, 0.004570809579380744, 0.004580452437435136, 0.0045685120496305786, 0.0045709734071025425, 0.004578777997641202, 0.004584042354343586, 0.004560433338237016, 0.004540095968501165, 0.004536098419686049, 0.004534925763194243, 0.004537777477791576, 0.0045366616179093175, 0.004535380070798597, 0.004548967516286017, 0.004538069860566191, 0.004528991068355675, 0.004514015126397005, 0.0045039897800375935, 0.0044863271483846345, 0.0044744313365666245, 0.004462372780962909, 0.004453511741071658, 0.004452187684373307, 0.004460026225954244, 0.004448735044671508, 0.004452701448999399, 0.004433498871088192, 0.004425734009233158, 0.004422034063353086, 0.004424264908972523, 0.004436939641156846, 0.004423786096134739, 0.004427637817271243, 0.004441679165268595, 0.004450577617069401, 0.004439083014262711, 0.004435652436419854, 0.004451322812517652, 0.004440388149653897, 0.00443298017239156, 0.004425708926279316, 0.004421302424498556, 0.004412987197851953, 0.0044252754819553205, 0.004430909563370202, 0.004444374896157123, 0.004441556529285699, 0.004436954631452555, 0.004431721574628593, 0.0044192722678882326, 0.00441114849919338, 0.004420250440719972, 0.004424219241907972, 0.004428998958110503, 0.004425544254409765, 0.004435126494079102, 0.004446643410544964, 0.00445056636514549, 0.004458345674403566, 0.004473303494488379, 0.004468961985415528, 0.0044792134904605456, 0.004467901633093507, 0.004472387152808325, 0.004474892574695764, 0.004473603279736853, 0.004467784218526393, 0.004465030191922222, 0.0044583867909284485, 0.00445950320288258, 0.004461408309404735, 0.004453030235098216, 0.004475704019190556, 0.004479331870425134, 0.004474928242822895, 0.004456541118590066, 0.004438324007179267, 0.004441695277687418, 0.004425834195826073, 0.00442600452967695, 0.004436439967578616, 0.0044276396829729964, 0.004430048129123271, 0.00443767657937859, 0.004447047113917175, 0.004445091836287663, 0.004442935068054854, 0.004452912299803847, 0.004450747497157993, 0.004448198582652261, 0.004445262216986078, 0.004439876202155959, 0.004446770058369765, 0.0044403000409645585, 0.0044327762515103545, 0.004427907381551072, 0.004424253710720736, 0.004417645049503274, 0.004414944476555933, 0.0044340358456401415, 0.004437168528025564, 0.0044438461601552836, 0.004455700164406236, 0.004462010604249122, 0.004462109430592106, 0.004470873379560395, 0.004476173688022048, 0.004482192116813796, 0.004477585081492834, 0.004460589794866618, 0.0044763931405896115, 0.0044730155168868895, 0.004479840773956014, 0.004495843039459849, 0.004496478507012617, 0.004490391717974542, 0.004486405661999127, 0.004492654735334795, 0.004489951293752912, 0.0045061723642468585, 0.004512710549141658, 0.004515609967344196, 0.0045046125437187895, 0.004492102037164519, 0.004512026442476868, 0.004517648087451133, 0.004524705709831002, 0.0045380148355408986, 0.004533502579315253, 0.00453566607966019, 0.004548733034255302, 0.004545402873945791, 0.0045469781269817915, 0.004555401676567402, 0.004557669600538239, 0.0045605710582123545, 0.004575312381152969, 0.004584812046122357, 0.004584444712887034, 0.00460721069723496, 0.004638816112346325, 0.004625726830314124, 0.004630819959569167, 0.004622533825629579, 0.004632368862040078, 0.00462990986923076, 0.004612603112948526, 0.0045996720965678336, 0.004604186065365253, 0.004600498226545157, 0.004614152713330957, 0.004622142611443929, 0.004630333183841135, 0.004622010329201293, 0.004636424576880397, 0.004646277609419898, 0.004632066177681307, 0.004628878322262853, 0.0046303186102690215, 0.004644194320670673, 0.00465329697534253, 0.004639519364137462, 0.00465407534580708, 0.004645051419910714, 0.004657446805997619, 0.004672018504981604, 0.004689817899483514, 0.004684839849934126, 0.004678250021962478, 0.004666351275962514, 0.004678772309157052, 0.004684382722925547, 0.0046943496324661155, 0.004692374168318379, 0.004684021463638827, 0.004678159865050869, 0.0046874061008074334, 0.004685480943766593, 0.004690151341643394, 0.004695989425184379, 0.004694581377286786, 0.004693783701829835, 0.004693855346082424, 0.004714133955297653, 0.004724218767677833, 0.004733634583116502, 0.004742127864430316, 0.004745324065244005, 0.004745901559033379, 0.004752977041945946, 0.004746736706799448, 0.0047565889763616365, 0.004749091071040471, 0.0047520729999386635, 0.004747374380744067, 0.004763542986740039, 0.004775044676293923, 0.004771928191440452, 0.0047733445490277405, 0.00476850890147031, 0.004767875432204985, 0.004774414309302141, 0.0047733906785936515, 0.004784310226726915, 0.0047829875392302115, 0.004783628442820516, 0.004766630863954712, 0.0047571219709899625, 0.004751855765208138, 0.0047634437013772965, 0.0047466448962199224, 0.004737246182577276, 0.004723751933891007, 0.004715635523516786, 0.004710705055278631, 0.00472353504994283, 0.004727915128326383, 0.004735420318880929, 0.004729810133769111, 0.00473458277881613, 0.004722350857179984, 0.004716791539329439, 0.004718539224386667, 0.004729613540543301, 0.004735315962190732, 0.004748279447644665, 0.00475790883174061, 0.004769532973218266, 0.0047720899669869785, 0.004774283161598545, 0.004771057003952746, 0.004766404869715667, 0.004778292511377845, 0.004773196855244438, 0.004777818186473551, 0.004782239011812361, 0.004772405590497793, 0.004768235462936956, 0.004769210911420089, 0.004778106646110027, 0.004792923764347476, 0.004792793644922381, 0.00479922119420832, 0.004808103976326012, 0.00481201839330166, 0.004816274907985337, 0.004812599491995754, 0.00482728050531563, 0.004815449807879783, 0.0047988762846097825, 0.004803381016086648, 0.0048003658609421865, 0.004784373610176278, 0.0047924389005633795, 0.004787211777023157, 0.004796441496853206, 0.0047931506671742355, 0.004793888055293258, 0.004797045083473581, 0.004793710341990333, 0.004785001898165457, 0.004777023022426451, 0.004776670393341201, 0.004755088362152965, 0.004743864023609863, 0.004738337582679496, 0.004741367890525117, 0.004740164500532232, 0.004728646551798041, 0.004734417025383521, 0.004725206414089568, 0.004736866423275006, 0.00475649012131093, 0.004746508319267372, 0.00474392817575535, 0.00474016379545483, 0.004749089839429034, 0.004744376982761491, 0.004753511046438014, 0.0047549676981358995, 0.004767256609317144, 0.00477671282025818, 0.004780456325906145, 0.004760847450459058, 0.004759872850520776, 0.004767606231376111, 0.004759265493864642, 0.004757792390585162, 0.004759166818273023, 0.00476255303346589, 0.004750571467517312, 0.004759081133168058, 0.004752649332868541, 0.004751009782967097, 0.004756013870736913, 0.004746146459407034, 0.004750658301041351, 0.004753879070591274, 0.004757721535352392, 0.004749135875374838, 0.004751217755576593, 0.004737722325435882, 0.004733085930043212, 0.004725630588361183, 0.004727847257720541, 0.004733352646665732, 0.00473400662969021, 0.0047322375773175135, 0.004738966140930066, 0.004744817031102404, 0.004737534210063824, 0.004731491321331666, 0.004747910419959851, 0.004744871440032883, 0.004737310829579157, 0.004735577232032554, 0.004724765535298234, 0.0047232694446707885, 0.004718037676419754, 0.0047252381868021385, 0.004725096316909882, 0.004714460003653521, 0.004720043507915084, 0.004714387295972219, 0.004702299602225061, 0.0047072506541056425, 0.004710250024589762, 0.004702021690519591, 0.004692925149308946, 0.004692977412268722, 0.004688261363015495, 0.00468849298810523, 0.004682294179453325, 0.004674202392928691, 0.004665237071791557, 0.004665186407863682, 0.0046673369192401165, 0.004657239998139493, 0.004659853571340578, 0.0046603597989504225, 0.004649257344265839, 0.004642182238036236, 0.004634590104822856, 0.004648899816581758, 0.004644825496545418, 0.004641379709207103, 0.004639758604786277, 0.004637859550068252, 0.0046265040824812935, 0.004621329549861838, 0.004623348078084091, 0.004639635994326834, 0.004659341409602147, 0.004659010285454899, 0.004661127479071217, 0.004663606832232748, 0.004661991630287462, 0.004672460778791674, 0.004671201116647863, 0.004678458636688867, 0.004679741050218162, 0.004685019650999454, 0.004675878443409277, 0.004681424226915957, 0.0046811367179014456, 0.004677777116389503, 0.004678417903029659, 0.0046874594068418325, 0.004682528517363414, 0.004669139489521364, 0.0046740573412653716, 0.00466591713665125, 0.004651539248098028, 0.004663249137464523, 0.0046593358465396245, 0.004659950094792291, 0.004655042799788726, 0.0046477582001739565, 0.004649862495439635, 0.004646590255397481, 0.004646137152490656, 0.004635762990325295, 0.004647435492698391, 0.004651133097078454, 0.004641951440174725, 0.00463420227639558, 0.004637981598677237, 0.004640338321877798, 0.004644645834934956, 0.004632908723600365, 0.004631203519730427, 0.004624316065975158, 0.004613257394593458, 0.004610306749698593, 0.004612497142238128, 0.004616873020844615, 0.00461125934666428, 0.004607759242638031, 0.004608358992917315, 0.004605833520331999, 0.0045961646554967435, 0.004593376205736198, 0.004595193964008745, 0.004589449489893463, 0.00459027555361105, 0.004593491960182488, 0.004604729065311168, 0.004605444314084925, 0.004615425697480899, 0.004610628025693746, 0.004607150544446642, 0.00460854762615248, 0.004617336208965354, 0.004613607076736545, 0.004603545726625357, 0.004615788769292542, 0.0046099352824948425, 0.004601350484788099, 0.004605905403280849, 0.004615407583884834, 0.0046236251010874875, 0.004612027422501779, 0.004623966458468596, 0.00461850663068137, 0.004609189893740762, 0.004606801221582564, 0.004594878451792936, 0.004585553959517656, 0.004571188202249915, 0.0045659645116478765, 0.00457002729028829, 0.004568690225665609, 0.004558720607805441, 0.004560036755277725, 0.00455237710490117, 0.004549754887679616, 0.004554653812464179, 0.004563292221885422, 0.00455706442811243, 0.004548609517609551, 0.004552733681689514, 0.004546345771828965, 0.0045399175407307675, 0.004531907992514109, 0.0045257474407309, 0.004517629155614005, 0.004524094494528319, 0.004537275534674498, 0.004538057561130171, 0.004524756644279876, 0.00452704026300412, 0.0045147929962995785, 0.0045169387850461495, 0.004510395860524548, 0.004501983912761793, 0.004491777099280568, 0.004499294001187401, 0.004495447032470357, 0.0044874092310885835, 0.004473931760713818, 0.004482726876421923, 0.004485199241933287, 0.004486826819470743, 0.004490481726453651, 0.004494512160724371, 0.004498178945799124, 0.00448457180095273, 0.004480631993605786, 0.004476728818694999, 0.004475661044225121, 0.00446785134880074, 0.004466250018931479, 0.004476253331604074, 0.004477529632288186, 0.004476742536251042, 0.004476586401621333, 0.0044689280652583895, 0.004470757063864159, 0.00446413122661678, 0.004460198290833451, 0.00446996174121647, 0.004477539417387159, 0.004477583072154629, 0.0044733124287096295, 0.004474126951080133, 0.004459263528468348, 0.0044498916290718745, 0.004451791558155833, 0.00445718346443953, 0.004453696666949198, 0.004435787220004298, 0.004432727168084515, 0.004421572767861369, 0.0044161343707207305, 0.004409622224299963, 0.004415045519357677, 0.0044162500485181765, 0.004402598156357866, 0.004402345941715684, 0.004407412924235455, 0.004411087422272722, 0.004417540844178704, 0.004421243421667464, 0.00441574700038302, 0.004418461623744463, 0.004425885536460593, 0.004418363665330401, 0.004422346417968876, 0.004421258755192576, 0.004411759572092666, 0.004412784244312698, 0.0044042734960208696, 0.004410664767211075, 0.0043992823808097995, 0.004401830781485817, 0.0043990506635677605, 0.004412101581780644, 0.004426271933210768, 0.004430333674303617, 0.004421079083879687, 0.004420074444356783, 0.004416118607509871, 0.004416244064080806, 0.004417008202157012, 0.004415798835641232, 0.004407355620569391, 0.004413668216567452, 0.004414768953433716, 0.004422942000774177, 0.004433228883926854, 0.004431203125820447, 0.00443199020834564, 0.004440923705631167, 0.004440105845937182, 0.004429575352537701, 0.004429816726188722, 0.004438868805552065, 0.0044337497310230145, 0.004433848304244737, 0.004422828239504366, 0.004427869345665111, 0.0044350964886129595, 0.004431519084295749, 0.004432510932993316, 0.0044228493932506495, 0.004419630945805255, 0.0044350351776617294, 0.004432351933594308, 0.004445384978784167, 0.004437494861736827, 0.004442240175139012, 0.00442611463873491, 0.00443824990636228, 0.004451614334890174, 0.00444453888678434, 0.00445990273083486, 0.004460302818426472, 0.0044702778845858515, 0.004470379257519891, 0.004471971711303795, 0.004466442828406254, 0.004466161521622203, 0.004468397998371232, 0.004474243878733628, 0.004472676424257163, 0.004456525964691821, 0.004457183915929475, 0.004461177518534177, 0.004462620773193871, 0.004468910625358708, 0.004464025921599264, 0.004473127323525436, 0.004472944028588509, 0.00446968074729247, 0.00446838487707071, 0.004468292819709575, 0.004455977727360924, 0.00444649908622633, 0.004432478446883243, 0.004434720019798479, 0.004435378950556701, 0.004438668115659299, 0.004433125462404171, 0.004436161187540182, 0.00444563432947545, 0.004447847811937677, 0.004445695350049155, 0.004437641214618119, 0.004437854728965351, 0.004436895603459319, 0.004438447353988106, 0.004437916764266083, 0.004439003830861783, 0.004432129460447543, 0.004430879198573753, 0.004438959757455163, 0.00443955666265011, 0.004440881261983391, 0.004441065917393676, 0.004449106548853981, 0.004454201093025128, 0.004448124767331792, 0.004442643641147562, 0.004433847133623335, 0.004431545570443309, 0.004422183578919544, 0.004424938626464683, 0.004425692219972141, 0.004428169442256149, 0.00442934052903486, 0.004429677362158592, 0.004433301097134917, 0.0044240382799569005, 0.004425869332447583, 0.004428643466585338, 0.004425377797459182, 0.004415794111768236, 0.00441648686879667, 0.004426599125141424, 0.004429437242859956, 0.004418623214010551, 0.004409529911229351, 0.00440259859951185, 0.004402869675160055, 0.004407653646275109, 0.004415187590957383, 0.00441907926248213, 0.004419230206412669, 0.004431218317014787, 0.004429170455197379, 0.004434456118813143, 0.004436936490164724, 0.00443734677482803, 0.004436127338982085, 0.004435126521181609, 0.004431407281858312, 0.004431231120650789, 0.0044400151970529576, 0.004434293016199333, 0.004424203109683554, 0.004426655651069448]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to NoneType.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12492\\2623645567.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                 })\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mlgbBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutility_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, event)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_subscribers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, event, instance)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                 \u001b[0mcolour\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpurple\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_new_max\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mColours\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                 \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolour\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolour\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_END\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_header_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, instance, colour)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterations\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_constrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mcells\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_bool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"allowed\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bayes_opt\\logger.py\u001b[0m in \u001b[0;36m_format_number\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 )\n\u001b[0;32m     41\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             s = \"{x:<{s}.{p}}\".format(\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_cell_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to NoneType.__format__"
     ]
    }
   ],
   "source": [
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 4000),\n",
    "                                                'max_depth': (5, 63),\n",
    "                                                'lambda_l2': (0.0, 0.05),\n",
    "                                                'lambda_l1': (0.0, 0.05),\n",
    "                                                'min_child_samples': (50, 10000),\n",
    "                                                'min_data_in_leaf': (100, 2000)\n",
    "                                                })\n",
    "\n",
    "lgbBO.maximize(n_iter=10, init_points=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rdkxhhST-KZS"
   },
   "source": [
    " **<font color='teal'> Print the best result by using the '.max' function.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:49:01.513767Z",
     "start_time": "2019-04-22T15:49:01.509392Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "oc8z6mfy-KZS"
   },
   "outputs": [],
   "source": [
    "lgbBO.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:50:29.049881Z",
     "start_time": "2019-04-22T15:50:29.045908Z"
    },
    "colab_type": "text",
    "id": "J5LAydKC-KZW"
   },
   "source": [
    "Review the process at each step by using the '.res[0]' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-22T15:51:01.001688Z",
     "start_time": "2019-04-22T15:51:00.997484Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "X1ttZmrI-KZX"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19740\\781366193.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlgbBO\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "lgbBO.res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Bayesian_optimization_exercise.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
